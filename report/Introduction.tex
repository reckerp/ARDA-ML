

\section{Introduction}
The first chapter of the given investigation will provide the guiding research question that will be accompanied by background information based on the given topic. Expanding on this foundation, a hypothesis will be stated, that shall thereafter be tested during the conducted experiment, for the given investigation.  lalalalal

\subsection{Research Question}
How does the number of training iterations a machine learning algorithm is trained with affect the object detection accuracy of a computer vision program?

\subsection{Context and Background}
Computer vision, a rapidly growing field found in artificial intelligence, that enables computer systems to interpret visual data through the use of machine learning. It aims to replicate the human visual system, allowing it to identify objects found in images, pictures or live video feeds. The increased adoption of computer vision in various fields, such as medicine, automotive, education, security or agriculture, underlines its widespread utilization and adoption. This is primarily reasoned due to its significant impact on efficiency, effectiveness and the enhancement of various applications found in these fields. Hence its importance and impact on the future are continuously increasing. 

\subsubsection{Computer vision}
To comprehend how computer vision systems function, it is vital to comprehend the correlation they have with the human biological vision system. This is because computer vision systems follow the same principles as the human visual system. It is hypothesised by neurobiologists, that the brain aims to detect patterns it is familiar with to decode into objects. Hence the brain uses the eyes as sensors, capturing the surroundings to create a visual representation. The brain then analyses the representation for recognizable patterns, creating predictions about the objects present. The given familiar patterns are derived from previous encounters with similar objects(further explained in 1.2.2). Computer vision models aim to replicate this process, utilising the camera to create images of real-world properties. Following image processing, the data undergoes analysis to identify familiar patterns, allowing confident predictions about the objects depicted in the image, to be made \parencite{Hohman2020}.

\subsubsection{Machine learning }
Machine learning, within the field of artificial intelligence, is an approach that enables a program to utilise historical data to learn and train with, by recognising patterns in data. This allows the program to make future predictions by drawing upon the knowledge acquired during training with past historical data. Consequently, the program gains the ability to generate independent predictions without being explicitly programmed for specific predictions. This process replicates the biological approach of a child learning the names of new objects. When a child is confronted with an unfamiliar object, the child may attempt to predict what the object could be based on similarities with a known object, but won't be able to accurately identify the object. However,  if the child is given the name of the object and is exposed to multiple instances of the given object, the child will start learning and become more adept at recognizing and identifying the object when encountered again. Machine learning functions on the same basis. The more the program is trained, the more refined its predictions become, as it accumulates experiences in accurately predicting specific objects. This learning process, also referred to as machine learning training, enhances the program's ability to create accurate predictions over time. 

\subsubsection{Importance of accuracy in computer vision programs}
With the widespread integration of computer vision programs across various fields, the accuracy of the given models assumes a key significance. Tesla, an automotive market leader for self-driving cars, has widely adopted computer vision programs based on machine learning for their autonomous vehicles. It is crucial for the given programs to accurately identify objects with a high level of confidence in real-time, enabling it to create a picture of the environment it is operating in. Hence resulting in a close-to-perfect detection accuracy. The significance lies in the fact that a failure to identify an object, can result in real-world consequences such as a collision with a different object. Therefore accuracy is a pivotal aspect of computer vision programs for widespread acceptance. Furthermore recognising that a low accuracy will directly translate into high error rates in programs utilising computer vision and retrospectively resulting in potential catastrophic implications. Therefore it can be stated that the accuracy of computer vision programs based on machine learning plays a pivotal role in the practical implementation of such models in real-world applications. 


\subsection{Hypothesis}
Based on the context provided, it can be hypothesised that increasing the training iterations for the machine learning algorithm will have a direct correlation with the accuracy of the object detection model. Hence the greater the number of training iterations the program goes through, the greater the accuracy should become. This hypothesis is based on the fact that each additional iteration of training, will allow the model to converge towards a more optimised state by adjusting its parameters to minimize the differences between the training and the test data.\\


It is expected that during the initial training phases of the given experiment, specifically between 1,000 and 3,000 iterations, the model might face challenges in identifying objects in the test images accurately. In the cases where it does recognise the given objects, it is probable that it will do so with only a minimal degree of accuracy. This is thereafter expected to greatly change as the model rapidly begins to gain a basic understanding of the objects it is confronted with based on its training. Therefore it is appropriate to assume that during the stage, between  4,000 and 13,000 the greatest increase in accuracy should be displayed.  Beyond this point, it can be expected that as the model reaches a substantial accuracy of around 80\%, it can be hypothesised that the growth in accuracy will steadily decrease as the model converges towards an optimal state and the accuracy plateaus.\\


Furthermore, it can be theorised that a decrease in the model's loss should lead to an increase in accuracy, considering the inverse relationship between loss and accuracy, where a loss reduction indicates an improved performance as loss is measured by testing for accuracy.\\


In summary, a significant correlation between the accuracy and the number of training iterations for the machine learning algorithm is expected, which will gradually decrease as the model approaches its optimal state.


